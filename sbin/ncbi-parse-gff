#!/usr/bin/env python

"""Write exonsets files from NCBI GFF alignments, as obtained from
ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/all_assembly_versions/GCF_*/
This service appeared in April 2015 and is due to update weekly.

See uta.formats for a description of those file formats.

In a nutshell, this means that you'll get data like this:

ncbi.exonsets.gz:
tx_ac   alt_ac  method  strand  exons_se_i
NM_130786.3 NC_000019.9 splign  -1  58864769,58864865;588646...
NM_130786.3 NC_018930.2 splign  -1  58858699,58858795;588585...
NM_130786.3 AC_000151.1 splign  -1  55173924,55174020;551738...
NM_138933.2 NC_000010.10    splign  -1  52645340,52645435;52...

UTA requires that the exon structure of a transcript accession as
defined on its own sequence is unique. Although this is mostly true,
there are instances where NCBI reports different exon structures for a
single transcript. For example, NM_001300954.1 aligns with 11 exons on
NC_000011.9 and 5 exons on NW_003871081.1, and the differences are NOT
due merely to concatenation of adjacent spans.  This script warns
vaguely about this problem 

"""


from __future__ import division

import argparse
import collections
import gzip
import importlib_resources
import io
import itertools
import logging.config
import os
import pprint
import re
import sys

import attr
import prettytable

from uta.formats.exonset import ExonSet, ExonSetWriter
from uta.formats.txinfo import TxInfo, TxInfoWriter, TxInfoReader
from uta.formats.geneaccessions import GeneAccessionsReader
from uta.tools.file_utils import open_file

origin = "NCBI"


@attr.s(slots=True)
class ExonAlignment(object):
    """alignment of a single exon"""
    ref_ac = attr.ib()
    origin = attr.ib()
    match_type = attr.ib()
    g_start = attr.ib(converter=int)
    g_end = attr.ib(converter=int)
    score = attr.ib()
    strand = attr.ib()
    aln_id = attr.ib()
    tx_ac = attr.ib()
    tx_start = attr.ib(converter=int)
    tx_end = attr.ib(converter=int)
    pct_coverage = attr.ib(converter=float)
    pct_identity_gap = attr.ib(converter=float)
    pct_identity_ungap = attr.ib(converter=float)


@attr.s(slots=True)
class TranscriptAlignment(object):
    """Represents a set of ExonAlignments for a single alignment id in tx exon start order"""

    exon_alignments = attr.ib()

    def __getitem__(self, sl):
        return self.exon_alignments.__getitem__(sl)

    def __len__(self):
        return len(self.exon_alignments)

    @property
    def tx_exons_se_i(self):
        return ";".join("{s},{e}".format(s=ea.tx_start-1, e=ea.tx_end) for ea in self.exon_alignments)

    @property
    def ref_exons_se_i(self):
        return ";".join("{s},{e}".format(s=ea.g_start-1, e=ea.g_end) for ea in self.exon_alignments)

    @property
    def tx_lens(self):
        return [(ea.tx_end-ea.tx_start+1) for ea in self.exon_alignments]

    @property
    def ref_lens(self):
        return [(ea.g_end-ea.g_start+1) for ea in self.exon_alignments]

    @property
    def ref_ac(self):
        return self.exon_alignments[0].ref_ac

    @property
    def origin(self):
        return self.exon_alignments[0].origin

    @property
    def score(self):
        return self.exon_alignments[0].score

    @property
    def strand(self):
        return self.exon_alignments[0].strand

    @property
    def aln_id(self):
        return self.exon_alignments[0].aln_id

    @property
    def tx_ac(self):
        return self.exon_alignments[0].tx_ac

    @property
    def pct_coverage(self):
        return self.exon_alignments[0].pct_coverage

    @property
    def pct_identity_gap(self):
        return self.exon_alignments[0].pct_identity_gap

    @property
    def pct_identity_ungap(self):
        return self.exon_alignments[0].pct_identity_ungap


def parse_args():
    ap = argparse.ArgumentParser(
        description=__doc__,
    )
    ap.add_argument("GFF_files", nargs="+",
                    help="NCBI GFF files to process")
    ap.add_argument("--origin", "-o",
                    default=origin)
    ap.add_argument("--prefix", "-p",
                    default="ncbi-gff")
    ap.add_argument("--strict-coverage",         "-C", type=float, default=95.0)
    ap.add_argument("--min-coverage",            "-c", type=float, default=85.0)
    ap.add_argument("--strict-pct-identity-gap", "-I", type=float, default=95.0)
    ap.add_argument("--min-pct-identity-gap",    "-i", type=float, default=85.0)

    opts = ap.parse_args()

    assert opts.strict_coverage > opts.min_coverage
    assert opts.strict_pct_identity_gap > opts.min_pct_identity_gap

    return opts


def read_exon_alignments(fn):
    """read lines of NCBI's alignment gff file, fn, returning ExonAlignment records"""

    # NC_000022.10    RefSeq  cDNA_match      20783512        20783627        116     -       .       ID=7b8c7a437b92bf9dee20d81acadadd8e;Target=NM_182895.5 1496 1611 +;consensus_splices=20;exon_identity=0.99769;for_remapping=2;gap_count=3;identity=0.99769;idty=1;matches=3455;num_ident=3455;num_mismatch=5;pct_coverage=99.9134;pct_coverage_hiqual=99.9134;pct_identity_gap=99.769;pct_identity_ungap=99.8555;product_coverage=1;rank=1;splices=20;weighted_identity=0.996898
    # NC_000022.10    RefSeq  cDNA_match      20781685        20781837        153     -       .       ID=7b8c7a437b92bf9dee20d81acadadd8e;Target=NM_182895.5 1612 1764 +;consensus_splices=20;exon_identity=0.99769;for_remapping=2;gap_count=3;identity=0.99769;idty=1;matches=3455;num_ident=3455;num_mismatch=5;pct_coverage=99.9134;pct_coverage_hiqual=99.9134;pct_identity_gap=99.769;pct_identity_ungap=99.8555;product_coverage=1;rank=1;splices=20;weighted_identity=0.996898
    # NC_000022.10    RefSeq  cDNA_match      20778874        20780569        1676.05 -       .       ID=7b8c7a437b92bf9dee20d81acadadd8e;Target=NM_182895.5 1765 3463 +;consensus_splices=20;exon_identity=0.99769;for_remapping=2;gap_count=3;identity=0.99769;idty=0.995291;matches=3455;num_ident=3455;num_mismatch=5;pct_coverage=99.9134;pct_coverage_hiqual=99.9134;pct_identity_gap=99.769;pct_identity_ungap=99.8555;product_coverage=1;rank=1;splices=20;weighted_identity=0.996898;Gap=M540 I1 M5 I1 M51 I1 M1100
    line_re = re.compile(
        "(?P<ref_ac>\S+)\s+(?P<origin>\S+)\s+(?P<match_type>\S+)\s+"
        "(?P<g_start>\d+)\s+(?P<g_end>\d+)\s+(?P<score>\S+)\s+"
        "(?P<strand>[-+])\s+\.\s+ID=(?P<aln_id>[^;]+);Target=(?P<tx_ac>\S+)"
        "\s+(?P<tx_start>\d+)\s+(?P<tx_end>\d+).+?"
        "pct_coverage=(?P<pct_coverage>[^;]+);.+?"
        "pct_identity_gap=(?P<pct_identity_gap>[^;]+);"
        "pct_identity_ungap=(?P<pct_identity_ungap>[^;]+)"
    )

    with open_file(fn) as fh:
        for line in fh:
            if not line.startswith('#'):
                try:
                    re_match = line_re.match(line)
                    if re_match and re_match["match_type"] == "cDNA_match":
                        yield ExonAlignment(**line_re.match(line).groupdict())
                except (AttributeError, ValueError):
                    raise Exception("Failed at", line)


def read_transcript_alignments(fn):
    """read an NCBI alignment gff, returning a generator of TranscriptAlignments
    that contains alignments grouped on id and sorted by tx_start

    This code assumes that alignments with the same alignment id occur
    in contiguous blocks in the file.

    """

    exon_alignments = read_exon_alignments(fn)
    return [TranscriptAlignment(exon_alignments=sorted(exons_i, key=lambda e: e.tx_start))
            for _, exons_i in itertools.groupby(exon_alignments, key=lambda e: e.aln_id)]


def group_transcript_alignments(transcript_alignments):
    """group transcript_alignments by tx_ac and ref_ac

    IMPORTANT: The gff file may contain multiple alignments for a
    <tx_ac, ref_ac> pair.  These pairs are not necessarily adjacent in
    the file. Therefore, the source must be materialized in order to
    sort and group transcript_alignments.

    """

    def _key(e):
        return (e.tx_ac, e.ref_ac)
    transcript_alignments = list(transcript_alignments)
    transcript_alignments.sort(key=_key)
    return ((key, list(alns_i))
            for key, alns_i in itertools.groupby(transcript_alignments, key=_key))


def convert_exon_data(transcript_alignment):
    """return (TxInfo,ExonSet) tuple for given exon record data"""
    es = ExonSet(
        tx_ac=transcript_alignment.tx_ac,
        alt_ac=transcript_alignment.ref_ac,
        method="splign",
        strand=-1 if transcript_alignment.strand == "-" else 1,
        exons_se_i=transcript_alignment.ref_exons_se_i
    )
    return es


def write_exonsets_from_gff_file(gff_fn, logger, opts, esw):
    """write exonsets from a single gff file"""
    ac_in_source = set()
    ac_failed = set()

    bins = "unique multiple minimum none skipped".split()
    sets = collections.defaultdict(lambda: {k: list() for k in bins})

    transcript_alignments = read_transcript_alignments(gff_fn)
    logger.info(
        "read {} transcript alignments from {}".format(len(transcript_alignments), gff_fn))

    for _, txalns in group_transcript_alignments(transcript_alignments):
        assert len(txalns) > 0

        ta0 = txalns[0]
        tx_ac, ref_ac = ta0.tx_ac, ta0.ref_ac
        skey = "{:.2s} {:.2s}".format(tx_ac, ref_ac)
        if not tx_ac[:2] in ("NM", "NR") or not ref_ac[:2] == "NC":
            sets[skey]["skipped"] += [txalns]
            continue

        bin = None
        txalns_load = []

        # ############################################################
        # Filter alignments by coverage and pct_identity_gap
        # From Terence Murphy, NCBI:
        # "For remapping variation, we typically only use transcript
        # and RefSeqGene alignments that meet the filter:
        # 'pct_identity_gap >= 99.5 and pct_coverage >= 95'"
        txalns_strict = [txaln
                         for txaln in txalns
                         if (txaln.pct_coverage > opts.strict_coverage
                             and txaln.pct_identity_gap > opts.strict_pct_identity_gap)]

        if len(txalns_strict) == 1:

            txalns_load = txalns_strict
            bin = "unique"

        elif len(txalns_strict) > 1:

            logger.warning("{ta.tx_ac}~{ta.ref_ac}: Multiple ({n}) strict alignments; cov/pig: {stats}".format(
                ta=txalns_strict[0], n=len(txalns_strict), opts=opts,
                stats="; ".join("{ta.pct_coverage}/{ta.pct_identity_gap}".format(ta=ta) for ta in txalns_strict),
            ))
            txalns_load = txalns_strict
            bin = "multiple"

        if len(txalns_strict) == 0:

            txalns_min = [txaln
                          for txaln in txalns
                          if (txaln.pct_coverage > opts.min_coverage
                              and txaln.pct_identity_gap > opts.min_pct_identity_gap)]
            if len(txalns_min) == 0:
                logger.warning("{ta.tx_ac}~{ta.ref_ac}: No usable alignments; cov/pig: {stats}".format(
                    ta=txalns[0],
                    stats="; ".join("{ta.pct_coverage}/{ta.pct_identity_gap}".format(ta=ta) for ta in txalns),
                ))
                bin = "none"
                ac_failed.add(skey)
            else:
                logger.warning(
                    "{ta.tx_ac}~{ta.ref_ac}: Resorting to minimum criteria; loading {n} alignments; cov/pig: {stats}".format(
                        ta=txalns_min[0], n=len(txalns_min),
                        stats="; ".join("{ta.pct_coverage}/{ta.pct_identity_gap}".format(ta=ta) for ta in txalns_min),
                    ))
                bin = "minimum"
            txalns_load = txalns_min

        sets[skey][bin] += [txalns]

        for ta in txalns_load:
            es = convert_exon_data(ta)
            ac_in_source.add(tx_ac)

            esw.write(es)

    # END HEINOUS LOOP

    seen_but_failed = ac_failed - ac_in_source
    if seen_but_failed:
        logger.warning("{n_acv} acvs seen but failed criteria: {acs}".format(
            n_acv=len(seen_but_failed), acs=",".join(sorted(seen_but_failed))))

    pt = prettytable.PrettyTable(field_names=["ac_pair"]
                                             + bins
                                             + "max_coverage max_pct_identity_gap nones".split()
                                 )
    for ack in sorted(sets.keys()):
        n = 5
        nones = list(itertools.chain.from_iterable(sets[ack]["none"]))
        nones_acs = sorted(set(ta.tx_ac for ta in nones))[:n]
        max_pct_identity_gap = "{:.2f}".format(max(ta.pct_identity_gap for ta in nones)) if nones else "n/a"
        max_pct_coverage = "{:.2f}".format(max(ta.pct_coverage for ta in nones)) if nones else "n/a"

        pt.add_row([ack] + [len(sets[ack][bk]) for bk in bins] +
                   [max_pct_coverage, max_pct_identity_gap,
                    " ".join(nones_acs)])
    logger.info("summary in table below...\n" + str(pt))



if __name__ == "__main__":
    logging_conf_fn = importlib_resources.files("uta").joinpath("etc/logging.conf")
    logging.config.fileConfig(logging_conf_fn)
    logging.getLogger().setLevel(logging.INFO)
    logger = logging.getLogger(__name__)

    opts = parse_args()

    esw = ExonSetWriter(sys.stdout)

    for gff_fn in opts.GFF_files:
        logger.info("processing {}".format(gff_fn))
        write_exonsets_from_gff_file(gff_fn, logger, opts, esw)
